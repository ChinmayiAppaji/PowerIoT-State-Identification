{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing required packages\n",
        "\n",
        "from math import ceil\n",
        "import math\n",
        "import numpy as np\n",
        "import calendar\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import json\n",
        "\n",
        "from numpy.lib.function_base import average\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import matplotlib.lines as mlines\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "calendar.setfirstweekday(6)"
      ],
      "metadata": {
        "id": "iKiZx5OQ_4bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your Data Files Path here\n",
        "data_files_path = '/content/gdrive/MyDrive/Data Files/Release Data'"
      ],
      "metadata": {
        "id": "cKO7Xmru_vlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset path into variables\n",
        "Location = 'India-1'\n",
        "harmonics_data = data_files_path + f'/Harmonics data/{Location}/{Location}_harmonics.csv'\n",
        "power_data = data_files_path + f'/Power consumption data/{Location}/{Location}.csv'"
      ],
      "metadata": {
        "id": "_IJlbpFVMmEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a Custom Pallete, that we will use in the later parts for representing the Colors of the States Identified\n",
        "sns.set(rc={'figure.figsize':(20,8)})\n",
        "\n",
        "palette = {\n",
        "    '0': list(sns.color_palette())[0],\n",
        "    '1': list(sns.color_palette())[1],\n",
        "    '2': list(sns.color_palette())[2],\n",
        "    '3': list(sns.color_palette())[3],\n",
        "    '4': list(sns.color_palette())[4],\n",
        "    '5': list(sns.color_palette())[5],\n",
        "    '6': list(sns.color_palette())[6],\n",
        "    '7': list(sns.color_palette())[7],\n",
        "}\n"
      ],
      "metadata": {
        "id": "loto5GF1b86R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the dataset CSV files. Function return a Dataframe object with datetime set as Index\n",
        "# Please change the Time Zone convertor based on the dataset being used (US/Eastern or Asia/Kolkata)\n",
        "def preprocessing_data( path ):\n",
        "\n",
        "    file_path = Path(path)\n",
        "    print(file_path)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df.rename(columns={'Unnamed: 0': 'datetime'})\n",
        "    df['datetime'] = pd.to_datetime(df['datetime']) \n",
        "    df = df.set_index('datetime')\n",
        "    # df = df.tz_convert('US/Eastern')\n",
        "    df = df.tz_convert('Asia/Kolkata')\n",
        "    df = df.resample('1min').mean()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "zQNaNbM9AG83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to obtain the Pricipal Components from the PCA algorithm\n",
        "# The function return a Dataframe with datetime as Index and multiple principle components as Columns of the dataframe\n",
        "def pca_function(df , components):\n",
        "\n",
        "  if 'ActivePT' in df.columns:\n",
        "    feature_array = df.drop( ['ActivePT'], axis=1 ).values\n",
        "  else:\n",
        "    feature_array = df.values\n",
        "\n",
        "  pca = PCA(n_components=components)\n",
        "  principalComponents = pca.fit_transform(feature_array)\n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['components_'+str(i+1) for i in range(components) ] )\n",
        "  principalDf.index = df.index\n",
        "\n",
        "  return principalDf"
      ],
      "metadata": {
        "id": "KBkhFi4JxWM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the power and harmonics data CSV files and storing them as DataFrames\n",
        "df_power = preprocessing_data(power_data)\n",
        "df_harmonics = preprocessing_data(harmonics_data)"
      ],
      "metadata": {
        "id": "RZ9wQVEx2RCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing and Interpolation\n",
        "In the following lines of code, we will preprocess the data.\n",
        "\n",
        "1. We choose only the necessary features from the Power and Harmonics DataFrames for usage in the following steps.\n",
        "2. We try to check how much of the datapoints are missing.\n",
        "3. We use interpolating functions to fill in the missing data."
      ],
      "metadata": {
        "id": "NVQNDrRaypg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the required features from Power and Harmonics DataFrames\n",
        "df_power = df_power[['ActivePT']]\n",
        "df_harmonics = df_harmonics[[ \"AI_HR3\", \"AI_HR5\", \"AI_HR7\", \"AI_HR9\", 'AI_HR11', 'AI_HR13', 'AI_HR15', 'AI_HR17', 'AI_HR19', 'AI_HR21', 'AI_HR23', 'AI_HR25', 'AI_HR27', 'AI_HR29', 'AI_HR31',\n",
        "                              \"BI_HR3\", \"BI_HR5\", \"BI_HR7\", \"BI_HR9\", 'BI_HR11', 'BI_HR13', 'BI_HR15', 'BI_HR17', 'BI_HR19', 'BI_HR21', 'BI_HR23', 'BI_HR25', 'BI_HR27', 'BI_HR29', 'BI_HR31', \n",
        "                              \"CI_HR3\", \"CI_HR5\", \"CI_HR7\", \"CI_HR9\", 'CI_HR11', 'CI_HR13', 'CI_HR15', 'CI_HR17', 'CI_HR19', 'CI_HR21', 'CI_HR23', 'CI_HR25', 'CI_HR27', 'CI_HR29', 'CI_HR31',\n",
        "                            ]]"
      ],
      "metadata": {
        "id": "7S_KA2z6N5zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the both the power and harmonics DataFrames into one df_merged\n",
        "# This cells also prints the percentage of the missing data.\n",
        "df_merged = pd.merge( df_harmonics, df_power, left_index=True, right_index=True)\n",
        "print(f'Missing data: {round(df_merged.isna().sum().sum()/(len(df_merged)*len(df_merged.columns)),4)*100}%')"
      ],
      "metadata": {
        "id": "mdc8OvT6pfHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to replace nan values - Functions for Interpolation\n",
        "\n",
        "def search_past_dates( data, timestamp, col ):\n",
        "  # print('Searching for values in past dates to ',timestamp)\n",
        "  start_timestamp = data.index[0]\n",
        "  end_timestamp = timestamp - timedelta(days=1)\n",
        "\n",
        "  time_value = timestamp.strftime('%H:%M:%S')\n",
        "\n",
        "  start_date = start_timestamp.date()\n",
        "  end_date = end_timestamp.date()\n",
        "\n",
        "  if start_date > end_date:\n",
        "    return False\n",
        "\n",
        "  period = (end_date - start_date).days + 1\n",
        "\n",
        "  for dt in pd.date_range( start_date, periods = period )[::-1]:\n",
        "    timestamp_str = str(dt.date()) +' ' + str(time_value)\n",
        "    timestamp_var = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S' )\n",
        "\n",
        "    if timestamp_str < start_timestamp.strftime( '%Y-%m-%d %H:%M:%S' ):\n",
        "      break\n",
        "\n",
        "    if timestamp_var not in data.index:\n",
        "      continue\n",
        "\n",
        "    value = data.loc[timestamp_str][col]\n",
        "\n",
        "    if math.isnan( value ) == False:\n",
        "      return timestamp_str\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def search_future_dates( data, timestamp, col ):\n",
        "  # print('Searching for values in future dates to ',timestamp)\n",
        "  start_timestamp = timestamp + timedelta(days=1)\n",
        "  end_timestamp = data.index[-1]\n",
        "\n",
        "  time_value = timestamp.strftime('%H:%M:%S')\n",
        "\n",
        "  start_date = start_timestamp.date()\n",
        "  end_date = end_timestamp.date()\n",
        "\n",
        "  if start_date > end_date:\n",
        "    return False\n",
        "\n",
        "  period = (end_date - start_date).days + 1\n",
        "\n",
        "  for dt in pd.date_range( start_date, periods = period ):\n",
        "    timestamp_str = str(dt.date()) +' ' + str(time_value)\n",
        "    timestamp_var = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S' )\n",
        "\n",
        "    if timestamp_str > end_timestamp.strftime( '%Y-%m-%d %H:%M:%S' ):\n",
        "      break\n",
        "\n",
        "    if timestamp_var not in data.index:\n",
        "      continue\n",
        "\n",
        "    value = data.loc[timestamp_str][col]\n",
        "\n",
        "    if math.isnan( value ) == False:\n",
        "      return timestamp_str\n",
        "\n",
        "  return False\n",
        "    \n",
        "# Fill the missing data in the dataframe - Interpolting the dataframe\n",
        "\n",
        "def interpolate_dataframe(df):\n",
        "  curr_month = 0\n",
        "  for idx in df.index:\n",
        "\n",
        "    if idx.date().month != curr_month:\n",
        "      curr_month = idx.date().month\n",
        "      # print(idx.date())\n",
        "\n",
        "    if df.loc[idx].isnull().values.any() == True:\n",
        "\n",
        "      if math.isnan( df.loc[idx]['ActivePT'] ) == False:\n",
        "        past_idx = search_past_dates( df, idx, 'AI_HR3' )\n",
        "        future_idx = search_future_dates( df, idx, 'AI_HR3' )\n",
        "\n",
        "        # print(idx, past_idx, future_idx)\n",
        "\n",
        "        if past_idx == False:\n",
        "          df.loc[idx, df.columns!='ActivePT'] = df.loc[future_idx, df.columns!='ActivePT']\n",
        "          continue\n",
        "\n",
        "        if future_idx == False:\n",
        "          df.loc[idx, df.columns!='ActivePT'] = df.loc[past_idx, df.columns!='ActivePT']\n",
        "          continue\n",
        "\n",
        "        df.loc[idx, df.columns!='ActivePT'] = (df.loc[past_idx, df.columns!='ActivePT'] + df.loc[future_idx, df.columns!='ActivePT'])/2\n",
        "\n",
        "      if math.isnan( df.loc[idx]['ActivePT'] ) == True:\n",
        "        past_idx = search_past_dates( df, idx, 'ActivePT' )\n",
        "        future_idx = search_future_dates( df, idx, 'ActivePT' )\n",
        "\n",
        "        # print(idx, past_idx, future_idx)\n",
        "\n",
        "        if past_idx == False:\n",
        "          df.loc[idx]['ActivePT'] = df.loc[future_idx]['ActivePT']\n",
        "          continue\n",
        "\n",
        "        if future_idx == False:\n",
        "          df.loc[idx]['ActivePT'] = df.loc[past_idx]['ActivePT']\n",
        "          continue\n",
        "\n",
        "        df.loc[idx] = (df.loc[past_idx] + df.loc[future_idx])/2\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Nzvp6er238AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the funtion for interpolating the missing data and checking for percentage of missing data after interpolation\n",
        "df_merged_filled = interpolate_dataframe(df_merged.copy())\n",
        "print(f'Missing data: {round(df_merged_filled.isna().sum().sum()/(len(df_merged_filled)*len(df_merged_filled.columns)),4)*100}%')"
      ],
      "metadata": {
        "id": "ZOaLaKOrsTwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The remaining NaN values (if any) are filled with the mean value\n",
        "if df_merged_filled.isnull().values.any():\n",
        "  df_merged_filled = df_merged_filled.fillna( df_merged_filled.mean() )\n",
        "df_merged_filled.isnull().values.any(), round(df_merged_filled.isna().sum().sum()/(len(df_merged_filled)*len(df_merged_filled.columns)),4)*100"
      ],
      "metadata": {
        "id": "os5a4jN8RTsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering and Classification\n",
        "\n",
        "In the following lines of Code, let us look at different functions that were modeled to obtain the States from the above processed Data\n",
        "\n",
        "1. We define different functions with the clustering algorithm and for training a classification model.\n",
        "2. Further we will see the implementation of elbow method, which we can use to define the optimal number of clusters that can be used."
      ],
      "metadata": {
        "id": "nTKDBql21zaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans Clustering Algorithm implemented in a function.\n",
        "# use 'return_centers = True' as an argument to get the centers of the clusters found.\n",
        "# By default the algorithm returns the labels of cluster the datapoint belongs to in a list and the inertia value from the model.\n",
        "def cluster_kmeans(data, n_clusters, return_centers=False):\n",
        "\n",
        "  model = KMeans(n_clusters=n_clusters, random_state=100000).fit(data)\n",
        "  centers = model.cluster_centers_\n",
        "  X_labels = model.labels_\n",
        "\n",
        "  if return_centers == True:\n",
        "    return [str(i) for i in X_labels], centers\n",
        "  else:\n",
        "    return [str(i) for i in X_labels], model.inertia_"
      ],
      "metadata": {
        "id": "LRSukglYu0si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following function return F1_Score and Silhouette Score, given the data, classification model and number of clusters.\n",
        "def get_f1_score( data, model, num_clusters ):\n",
        "  prediction_labels = model.predict(data)\n",
        "  kmeans_labels, _ = cluster_kmeans(np.array(data), num_clusters)\n",
        "\n",
        "  if len(set(kmeans_labels)) > 1:\n",
        "    cluster_silScore  = metrics.silhouette_score(np.array(data), kmeans_labels, metric='euclidean')\n",
        "  else:\n",
        "    cluster_silScore = 0\n",
        "    \n",
        "  f1_score = metrics.f1_score(kmeans_labels, prediction_labels, average='micro' )\n",
        "\n",
        "  return f1_score, cluster_silScore"
      ],
      "metadata": {
        "id": "TJmB0os7tJPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Classification model\n",
        "# Takes a dataframe and number of clusters as arguments to train the Random Forest Classifier model.\n",
        "# Function return a trained classification model\n",
        "\n",
        "def train_cluster_classification_model(  data, number_of_clusters ):\n",
        "\n",
        "  if 'ActivePT' in data.columns:\n",
        "    flag = 1\n",
        "  else:\n",
        "    flag = 0\n",
        "\n",
        "  labels = []\n",
        "  centers = []\n",
        "  if flag == 1:\n",
        "    X = np.array(data.drop(['ActivePT'], axis=1))\n",
        "  else:\n",
        "    X = np.array(data)\n",
        "\n",
        "  labels, centers_list = cluster_kmeans( X, number_of_clusters, return_centers=True )\n",
        "\n",
        "  data['cluster_labels'] = labels\n",
        "\n",
        "  if flag == 1:\n",
        "    train_X = data.drop(['ActivePT','cluster_labels'], axis=1)\n",
        "    train_y = data['cluster_labels']\n",
        "  else:\n",
        "    train_X = data.drop(['cluster_labels'], axis=1)\n",
        "    train_y = data['cluster_labels']\n",
        "\n",
        "  model = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
        "  model.fit(train_X,train_y)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "z5_f52q1pJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the Dates in a DataFrame\n",
        "dates_in_dataframe = set([])\n",
        "for i in df_merged_filled.index:\n",
        "  if i.date() not in dates_in_dataframe:\n",
        "    dates_in_dataframe.add(i.date())\n",
        "dates_in_dataframe = sorted(list(dates_in_dataframe))"
      ],
      "metadata": {
        "id": "HIIBxlD37hH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the start and end dates of the dataset\n",
        "df_merged_filled.index[0], df_merged_filled.index[-1]"
      ],
      "metadata": {
        "id": "JpXYNv13E6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Training Start Date and Training End Date\n",
        "train_start = f'{dates_in_dataframe[0].year}-{dates_in_dataframe[0].month}-{dates_in_dataframe[0].day}'\n",
        "train_end = f'{dates_in_dataframe[6].year}-{dates_in_dataframe[6].month}-{dates_in_dataframe[6].day}'"
      ],
      "metadata": {
        "id": "Ot-4zo1S8dT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code cell generates two plots\n",
        "# 1. Number of Clusters vs Inertia of the model\n",
        "# 2. Number of Clusters vs Silhouette Score\n",
        "# These both plots are used to examine and determine the optimal number of clusters, \n",
        "# which can be used further for clustering and classification\n",
        "fig,ax = plt.subplots(1,2, figsize=(10,4))\n",
        "\n",
        "inertia_list = []\n",
        "silhouette_list = []\n",
        "for k in range(2,20):\n",
        "  test_model = KMeans(n_clusters=k, random_state=100000).fit(df_merged_filled.loc[train_start:train_end].drop(['ActivePT'], axis=1))\n",
        "  inertia_list.append(test_model.inertia_)\n",
        "  silhouette_list.append(metrics.silhouette_score(np.array(df_merged_filled.loc[train_start:train_end].drop(['ActivePT'], axis=1)), test_model.labels_, metric='euclidean'))\n",
        "\n",
        "sns.lineplot( x = list(range(2,20)), y = inertia_list, ax=ax[0])\n",
        "ax[0].set_xlabel('Number of Clusters (Value of K)')\n",
        "ax[0].set_ylabel('Inertia')\n",
        "\n",
        "sns.lineplot( x = list(range(2,20)), y = silhouette_list, ax=ax[1])\n",
        "ax[1].set_xlabel('Number of Clusters (Value of K)')\n",
        "ax[1].set_ylabel('Silhouette Score')\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "RYupboll1FRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chosing the optimal number of clusters from the above plots\n",
        "choice_cluster_num = 6"
      ],
      "metadata": {
        "id": "prwAEfRF5ZSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining a trained classfication model.\n",
        "model = train_cluster_classification_model( data = df_merged_filled.loc[train_start:train_end], number_of_clusters = choice_cluster_num)"
      ],
      "metadata": {
        "id": "2gPVmwJ9mOm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the States found from the classification model for each date in the Dataset\n",
        "for date_item in dates_in_dataframe[7:]:\n",
        "  date = f'{date_item.year}-{date_item.month}-{date_item.day}'\n",
        "  test_df = df_merged_filled.loc[date]\n",
        "  prediction_labels = model.predict(test_df.drop(['ActivePT'], axis=1))\n",
        "  print(f'{date} - predicted_labels {set(prediction_labels)}')\n",
        "  state_details[Location]['states_found'][str(date)] = sorted(set(prediction_labels))"
      ],
      "metadata": {
        "id": "DDRxi2aCP12o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting (Scatter Plot) the states found using the classification model\n",
        "# with X-axis = datetime; and Y-axis = Active Power. \n",
        "# Hue argument of the seaborn library is used to represent different states found with different colors.\n",
        "for date_item in dates_in_dataframe[7:]:\n",
        "  date = f'{date_item.year}-{date_item.month}-{date_item.day}'\n",
        "  test_df = df_merged_filled.loc[date]\n",
        "  prediction_labels = model.predict(test_df.drop(['ActivePT'], axis=1))\n",
        "\n",
        "  f1_score, _ = get_f1_score(data = test_df.drop(['ActivePT'], axis=1), model = model, num_clusters = choice_cluster_num) \n",
        "  f1_score_dic[Location][date] = f1_score\n",
        "\n",
        "  fig, ax = plt.subplots(2,1, figsize=(12,5))\n",
        "\n",
        "  principalDf = pca_function(test_df, 2)\n",
        "\n",
        "  sns.scatterplot(data=test_df, x=test_df.index, y='ActivePT', hue=prediction_labels, palette = palette, ax=ax[0])\n",
        "  sns.scatterplot( data = principalDf, x=\"components_1\", y=\"components_2\", hue=prediction_labels, palette=palette,ax = ax[1])\n",
        "\n",
        "  ax[0].legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
        "  ax[1].legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
        "\n",
        "  ax[0].set_title('(a) Cluster with Active Power on Y-Axis').set_fontsize(15)\n",
        "  ax[0].set_xlabel('datetime')\n",
        "  ax[1].set_title('(b) PCA ').set_fontsize(15)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  # fig.savefig(f'{Location}_{date}')"
      ],
      "metadata": {
        "id": "k9738RTvmj6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}